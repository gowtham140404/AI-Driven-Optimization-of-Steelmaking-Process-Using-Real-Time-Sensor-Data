{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATmUSpL2Csqm"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "AI-Driven Optimization of Steelmaking Process Using Real-Time Sensor Data (Jan 2025 - Mar 2025)\n",
        "\n",
        "\n",
        "This pipeline demonstrates:\n",
        " - Loading and preprocessing real-time steelmaking sensor data\n",
        " - Training ML models (RandomForest, GradientBoosting, optionally XGBoost/LightGBM)\n",
        " - Predicting steel quality (classification/regression)\n",
        " - Optimizing key parameters (temperature, composition, flow) using a surrogate model\n",
        " - Visualizing results (feature importance, parameter optimization)\n",
        "\n",
        "Usage:\n",
        " python steelmaking_ai_optimization.py --data_file steel_sensor_data.csv --label_col quality\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGB_AVAILABLE = False\n",
        "\n",
        "# -------------------- Data Prep --------------------\n",
        "def load_and_prepare_data(file, label_col, test_size=0.2, scale=True):\n",
        "    df = pd.read_csv(file)\n",
        "    X = df.drop(columns=[label_col])\n",
        "    y = df[label_col]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "        X_test = scaler.transform(X_test)\n",
        "    else:\n",
        "        X_train, X_test = X_train.values, X_test.values\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, list(df.drop(columns=[label_col]).columns)\n",
        "\n",
        "# -------------------- Training --------------------\n",
        "def train_models(X_train, y_train):\n",
        "    models = {}\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=300, max_depth=None, class_weight=\"balanced\", random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    models[\"RandomForest\"] = rf\n",
        "\n",
        "    gb = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)\n",
        "    gb.fit(X_train, y_train)\n",
        "    models[\"GradientBoosting\"] = gb\n",
        "\n",
        "    if XGB_AVAILABLE:\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            n_estimators=400,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            eval_metric=\"mlogloss\",\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "        models[\"XGBoost\"] = xgb_model\n",
        "\n",
        "    return models\n",
        "\n",
        "# -------------------- Evaluation --------------------\n",
        "def evaluate_model(model, X_test, y_test, name):\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "\n",
        "    print(f\"\\n=== {name} Evaluation ===\")\n",
        "    print(\"Accuracy:\", round(acc, 4))\n",
        "    print(classification_report(y_test, preds))\n",
        "\n",
        "    cm = confusion_matrix(y_test, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "    return acc\n",
        "\n",
        "# -------------------- Optimization --------------------\n",
        "def optimize_parameters(model, feature_names, bounds, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Use the trained model to explore parameter space and suggest optimal values.\n",
        "    bounds: dict {feature: (min, max)}\n",
        "    \"\"\"\n",
        "    samples = {}\n",
        "    for feat, (low, high) in bounds.items():\n",
        "        samples[feat] = np.random.uniform(low, high, n_samples)\n",
        "    df_samples = pd.DataFrame(samples)\n",
        "\n",
        "    preds = model.predict(df_samples)\n",
        "    # assume higher quality class is max label\n",
        "    optimal = df_samples.loc[preds.argmax()] if hasattr(preds, \"argmax\") else df_samples.iloc[0]\n",
        "\n",
        "    print(\"\\nSuggested optimal parameters:\")\n",
        "    print(optimal)\n",
        "    return optimal\n",
        "\n",
        "# -------------------- Visualization --------------------\n",
        "def plot_feature_importances(model, feature_names, name, top_n=10):\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1][:top_n]\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.bar(range(top_n), importances[indices], align=\"center\")\n",
        "        plt.xticks(range(top_n), [feature_names[i] for i in indices], rotation=45, ha=\"right\")\n",
        "        plt.title(f\"{name} Top {top_n} Features\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# -------------------- Main --------------------\n",
        "def main(args):\n",
        "    X_train, X_test, y_train, y_test, feature_names = load_and_prepare_data(\n",
        "        args.data_file, args.label_col\n",
        "    )\n",
        "\n",
        "    models = train_models(X_train, y_train)\n",
        "    best_model, best_acc = None, 0\n",
        "\n",
        "    for name, model in models.items():\n",
        "        acc = evaluate_model(model, X_test, y_test, name)\n",
        "        plot_feature_importances(model, feature_names, name)\n",
        "        if acc > best_acc:\n",
        "            best_model, best_acc = model, acc\n",
        "\n",
        "    print(f\"Best model achieved accuracy: {round(best_acc, 4)}\")\n",
        "\n",
        "    # Optimization demo (example bounds)\n",
        "    bounds = {\n",
        "        \"temperature\": (1500, 1700),\n",
        "        \"composition_C\": (0.02, 0.1),\n",
        "        \"flow_rate\": (100, 300),\n",
        "    }\n",
        "    optimize_parameters(best_model, feature_names, bounds)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"AI-Driven Steelmaking Optimization\")\n",
        "    parser.add_argument(\"--data_file\", type=str, required=True, help=\"Path to sensor data CSV\")\n",
        "    parser.add_argument(\"--label_col\", type=str, required=True, help=\"Target column for quality\")\n",
        "    args = parser.parse_args()\n",
        "    main(args)"
      ]
    }
  ]
}